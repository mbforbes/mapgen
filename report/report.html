<!DOCTYPE html>
<html>
<head>
    <title>Max Forbes: CSE 557 Project Report</title>
    <link href="css/report.css" rel="stylesheet">
    <link href="css/code.css" rel="stylesheet">
</head>
<body>
<h1>A Machine Learning Approach <i>to</i> Procedural City Layout Generation</h1>

<p><em>Maxwell Forbes</em><br />
<em>CSE 557 Final Project Report</em><br />
<em>December 15, 2017</em></p>
<div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#task">Task</a></li>
</ul>
</li>
<li><a href="#related-work">Related Work</a><ul>
<li><a href="#inverse-procedural-modeling-and-shape-learning">Inverse Procedural Modeling and Shape Learning</a></li>
<li><a href="#city-modeling">City Modeling</a></li>
</ul>
</li>
<li><a href="#dataset-creation">Dataset Creation</a><ul>
<li><a href="#task-1-individual-blocks">Task 1: Individual Blocks</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#graph-ring-discovery">Graph Ring Discovery</a></li>
<li><a href="#block-building-matching-and-rendering">Block-Building Matching and Rendering</a></li>
</ul>
</li>
<li><a href="#task-2-map-regions">Task 2: Map Regions</a><ul>
<li><a href="#rendering-regions">Rendering Regions</a></li>
</ul>
</li>
<li><a href="#scraping">Scraping</a></li>
</ul>
</li>
<li><a href="#model">Model</a></li>
<li><a href="#experimental-results-and-discussion">Experimental Results and Discussion</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>
<h2 id="introduction">Introduction</h2>
<h3 id="motivation">Motivation</h3>
<p>Procedural generation affords the creation of large, authentic looking
environments with far less time and effort than manual modeling.</p>
<p>The domain of <em>urban</em> procedural generation can be roughly split into three
sub-problems: layout modelling (maps and roads), building modelling (3D
geometries), and facade modelling (3D facades and
textures).<sup id="fnref:vanegas2010modelling"><a class="footnote-ref" href="#fn:vanegas2010modelling" rel="footnote">1</a></sup> Exciting advances in all of these areas have
led to remarkably realistic results, such as generating cities that expand over
time<sup id="fnref:weber2009interactive"><a class="footnote-ref" href="#fn:weber2009interactive" rel="footnote">2</a></sup> and villages that grow based on their
geography.<sup id="fnref:emilien2012procedural"><a class="footnote-ref" href="#fn:emilien2012procedural" rel="footnote">3</a></sup></p>
<p>While procedural generation offers great gains compared to manually creating
content, a key problem still exists: the designers of such generation systems
create the algorithms by manually designing and encoding grammars that produce
realistic looking results. This process requires substantial domain expertise
and significant trial and error. In other words, procedural generation does not
remove the manual effort required to generate large cities; it simply shifts
the effort from 3D modeling to algorithm design!</p>
<p>However, for city layouts, freely available data from crowdsourced map projects
now exist. From this data, it may be possible to train a machine learning model
that can automatically generate city layouts without any hand-tuned grammar
rules.</p>
<p>In addition to simply removing the manual work of designing generation
algorithms, a machine learning model offers other pragmatic advantages to
virtual city creators. For example, a model would learn to generate cities in
the <em>style</em> of the geographic area on which it was trained. An old European
town will present different layout patterns than a bustling metropolis like
Tokyo. A machine learning model could capture these differences simply by being
retrained on different data, without needed to re-design the parameters or
grammars of the generation algorithm.</p>
<h3 id="task">Task</h3>
<p>In this project, we approach the first domain of urban procedural generation:
generating the layout for a city. Within this domain, we further restrict our
focus to the following task: given a road network and city features (like water
and parks), fill in the buildings inside of blocks. Figure TODO shows a visual
depiction of our task.</p>
<p>TODO: figure for task</p>
<p>We split this overall task into two subtasks. In the first subtask, we extract
individual blocks with their buildings laid out on top of them. The goal is to
generate the buildings for a single block at a time (Figure TODO (a)). This
problem is more constrained, and provides an early test of the model's
capabilities.</p>
<p>In the second subtask, we provide a larger chunk of a city as input, and ask
the model to fill in buildings in all the empty blocks provided (Figure TODO
(b)). This task is more difficult, because more buildings must be generated and
placed within the bounds of blocks. But because we also provide geographic
features like water and parks in the input, a model can potentially take
advantage of these semantic cues to generate layouts that are sensitive to
their surroundings.</p>
<p>TODO: figure both subtasks</p>
<h2 id="related-work">Related Work</h2>
<h3 id="inverse-procedural-modeling-and-shape-learning">Inverse Procedural Modeling <em>and</em> Shape Learning</h3>
<p>The idea of attempting to learn the parameters of a procedural generation model
is called <em>inverse procedural modeling</em>. Though this has never been applied to
city layout generation, it has been explored by several authors in other
domains.</p>
<p>Wu et al. (2014) learn a split grammar for facades, preferring shorter
descriptions as better representations of the grammar.<sup id="fnref:wu2013inverse"><a class="footnote-ref" href="#fn:wu2013inverse" rel="footnote">4</a></sup> Though
it does not appear to use machine learning, <em>Inverse Procedural Modeling by
Automatic Generation of L-Systems</em> (2010) propose an approach for
reverse-engineering the parameters for an L-system (grammar) given input (in
their case, 2D vector images) that was generated from one.<sup id="fnref:vst2010inverse"><a class="footnote-ref" href="#fn:vst2010inverse" rel="footnote">5</a></sup></p>
<p>Other authors have used graphical models to learn how to generate shapes and
textures. Fan and Wonka (2016) learned a garphical model to generate 3D
buildings and facades.<sup id="fnref:fan2016probabilistic"><a class="footnote-ref" href="#fn:fan2016probabilistic" rel="footnote">6</a></sup> Martinovic and Gool (2013) learn
a 2D context-free grammar (CFG) from a set of labeled
facades.<sup id="fnref:martinovic2013bayesian"><a class="footnote-ref" href="#fn:martinovic2013bayesian" rel="footnote">7</a></sup> In <em>A Probabilistic Model for Component-Based
Shape Synthesis</em> (2012), the authors learn a graphical model trained on
hand-modeled 3D shapes (like a dinosaur or a chair) in order to generate their
own novel meshes.<sup id="fnref:kalogerakis2012probabilistic"><a class="footnote-ref" href="#fn:kalogerakis2012probabilistic" rel="footnote">8</a></sup> Toshev et al. (2010) take
inspiration from classical natural language processing, and learn the
parameters of a parsing model to map point clouds that represent roofs to a
hierarchy of the roof's components (e.g., main roof, hood over window, shed
roof, etc.) <sup id="fnref:toshev2010detecting"><a class="footnote-ref" href="#fn:toshev2010detecting" rel="footnote">9</a></sup>.</p>
<h3 id="city-modeling">City Modeling</h3>
<p>Though these works do not use machine learning or inverse procedural
generation, it is worth briefly touching upon city generation literature.
Several papers present tools for editing or expanding a set of aerial images of
cities. Aliaga et al. (2008, a) present a tool for making edits to an urban
layout and generate roads and parcels to fit into the edited
regions.<sup id="fnref:aliaga2008interactiveA"><a class="footnote-ref" href="#fn:aliaga2008interactiveA" rel="footnote">10</a></sup> In a followup work, Aglia et al. <em>(2008, b)</em>
demonstrate another tool that, given an example image, synthesizes a new street
network, and pastes in segments of the input image that fit well with the new
road network <sup id="fnref:aliaga2008interactiveB"><a class="footnote-ref" href="#fn:aliaga2008interactiveB" rel="footnote">11</a></sup>.</p>
<p>Other work focuses on generating cities from scratch. Weber et al. (2009)
simulate a city's growth over time, taking into account population growth and
the according land use evolution.<sup id="fnref2:weber2009interactive"><a class="footnote-ref" href="#fn:weber2009interactive" rel="footnote">2</a></sup> In <em>Procedural
Generation of Parcels in Urban Modeling</em> (2012), the authors develop an
algorithm for automatically splitting blocks (the spaces carved out by road
networks) into parcels (areas of land ownership).<sup id="fnref:vanegas2012procedural"><a class="footnote-ref" href="#fn:vanegas2012procedural" rel="footnote">12</a></sup>
Finally, Nishida et al. (2016) present a tool for editing road networks that
takes into account the style and layout of example data.<sup id="fnref:nishida2016example"><a class="footnote-ref" href="#fn:nishida2016example" rel="footnote">13</a></sup></p>
<h2 id="dataset-creation">Dataset Creation</h2>
<p>To the best of our knowledge, no previous work attempt the task of generating
city layouts using machine learning. Because of this, a significant portion of
the project time was spent collecting and preprocessing the data. For that
reason, this section of the report gives a brief overview of this process.</p>
<h3 id="task-1-individual-blocks">Task 1: Individual Blocks</h3>
<p>The first task we address is: given a block, generate the buildings on the
block. For this task, we process maps data from OpenStreetMaps in order to
identify and extract blocks.</p>
<h4 id="overview">Overview</h4>
<p>The overall processes of the block extraction is shown below in figure TODO.</p>
<p><br /></p>
<p><img alt="dataset fig 1" src="img/datafig-1.png" />
<img alt="dataset fig 2" src="img/datafig-2.png" /></p>
<p><em>Figure TODO: Stages of block extraction, done for the first task. The
individual steps are described in the running text.</em></p>
<p><br /></p>
<p>Block extraction broadly involved the following stages, each of which are
illustrated above (Figure TODO):</p>
<ul>
<li>
<p>(a) OpenStreetMaps data is parsed from its native XML format, and all
  <em>ways</em> (collections of nodes) are rendered as polygons.</p>
</li>
<li>
<p>(b) Crowdsourced labels are aggregated into high-level features (such as
  buildings and roads) and polygons are colored according to their predominant
  feature.</p>
</li>
<li>
<p>(c) Roads are the only <em>ways</em> that should not be rendered as polygons; they
  are properly drawn as polylines.</p>
</li>
<li>
<p>(d) Nodes that serve as the underlying points for road <em>ways</em> are rendered.</p>
</li>
<li>
<p>(e) Road nodes are rendered in varying sizes to confirm that roads are drawn
  independently (i.e., informing us that intersections must be discovered).</p>
</li>
<li>
<p>(f) Nodes are recursively collapsed by finding nodes within a geographic
  euclidean distance and recursively building a map of backpointers.</p>
</li>
<li>
<p>(g) Now that nodes connect roads together, the map may be rendered as a
  graph, here shown in green and blue.</p>
</li>
<li>
<p>(i) First stage of block discovery: blocks of small distance (up to four
  edges) are discovered, but duplicates exist because the same block may be
  discovered by multiple nodes.</p>
</li>
<li>
<p>(j) Deduplication of identical blocks by keeping only unique sets of
  vertices. (Blocks are rendered with semi-transparency; the difference can be
  seen here from the last step because the blocks are a lighter shade of pink,
  indicating they only exist once.)</p>
</li>
<li>
<p>(k) Increasing the maximum search depth of the block discovery algorithm, we
  begin to find larger sets of nodes that encompass multiple blocks (darker
  pink regions). Because some blocks are defined by a large number of nodes due
  to having curvy roads, some are still missed.</p>
</li>
<li>
<p>(l) Further increasing the maximum block search depth, we recover all
  feasible blocks. At this point, heavy duplicate coverage plagues blocks due
  to the algorithm discovering many false enclosing blocks.</p>
</li>
<li>
<p>(m) Removal of false enclosing blocks. This is done by rendering all
  candidate blocks and removing any large candidates that fully enclose smaller
  candidates.</p>
</li>
</ul>
<h4 id="graph-ring-discovery">Graph Ring Discovery</h4>
<p>A crucial part of the above process was devising an algorithm to find rings in
the graph in order to identify candidate blocks. The algorithm is presented
below in Python-like pseudocode with types. It is an augmented breadth-first
search which tracks unique paths to vertices from paths starting at all
neighbors of a start vertex.</p>
<p><em>NB: While presenting this algorithm, I was pointed to a simpler algorithm that
takes into account the the geometry of the map: for each edge, follow edges of
a maximum angle (e.g., clockwise) until the starting vertex is reached. For
completeness, I still present here the algorithm that I devised to find rings
in a graph.</em></p>
<p><br /></p>
<div class="codehilite"><pre><span></span><span class="c1"># The overall algorithm searches from each vertex and returns the unique</span>
<span class="c1"># set of rings discovered.</span>
<span class="k">def</span> <span class="nf">find_rings</span><span class="p">(</span><span class="n">graph</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
    <span class="k">return</span> <span class="n">unique</span><span class="p">(</span><span class="n">find_rings_at</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>


<span class="c1"># The bulk of the algorithm finds all rings that involve a chosen vertex up</span>
<span class="c1"># to a maximum depth.</span>
<span class="k">def</span> <span class="nf">find_rings_at</span><span class="p">(</span><span class="n">graph</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">start</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">maxdepth</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
    <span class="c1"># Setup.</span>
    <span class="n">start_path</span> <span class="o">=</span> <span class="p">[</span><span class="n">start</span><span class="p">]</span>  <span class="c1"># type: List[int]</span>
    <span class="n">shortest</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># type: Dict[int, List[List[int]]]</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">([(</span><span class="n">start</span><span class="p">,</span> <span class="n">start_path</span><span class="p">)])</span>

    <span class="c1"># First, find sets of unique paths to surrounding vertices.</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Consider the vertex just found, a candidate &quot;shortest path.&quot;</span>
        <span class="n">cur</span><span class="p">,</span> <span class="n">curpath</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">can_add_to_shortest</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="n">curpath</span><span class="p">,</span> <span class="n">shortest</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">cur</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">shortest</span><span class="p">:</span>
                <span class="n">shortest</span><span class="p">[</span><span class="n">cur</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">shortest</span><span class="p">[</span><span class="n">cur</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curpath</span><span class="p">)</span>

        <span class="c1"># Add neighbors to queue if we haven&#39;t explored to max depth yet.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">curpath</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">maxdepth</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">neighbor</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">[</span><span class="n">cur</span><span class="p">]:</span>
                <span class="c1"># No backtracking per path.</span>
                <span class="k">if</span> <span class="n">neighbor</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">curpath</span><span class="p">:</span>
                    <span class="n">q</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">neighbor</span><span class="p">,</span> <span class="n">curpath</span> <span class="o">+</span> <span class="p">[</span><span class="n">neighbor</span><span class="p">]))</span>

    <span class="c1"># Now, extract rings. They are discovered vertices with multiple paths.</span>
    <span class="n">rings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">shortest</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">paths</span> <span class="o">=</span> <span class="n">shortest</span><span class="p">[</span><span class="n">candidate</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">paths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Use the first two paths found, and remove duplicate nodes</span>
            <span class="c1"># (first and last) from the second.</span>
            <span class="n">p1</span> <span class="o">=</span> <span class="n">paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">p2</span> <span class="o">=</span> <span class="n">paths</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">rings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p1</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">p2</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>

    <span class="k">return</span> <span class="n">rings</span>

<span class="c1"># This helper algorithm determines whether a candidate path should be added</span>
<span class="c1"># to the set of shortest paths to a vertex.</span>
<span class="k">def</span> <span class="nf">can_add_to_shortest</span><span class="p">(</span><span class="n">cur</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">curpath</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">shortest</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># If shortest hasn&#39;t found cur yet, then found a new shortest path.</span>
    <span class="k">if</span> <span class="n">cur</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">shortest</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="c1"># Crowdsourced map data; might have multiple edges between two vertices.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">curpath</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>

    <span class="c1"># Interesting case: We want to add only if we&#39;ve found a new path to</span>
    <span class="c1"># this node that is unique; i.e., the middle nodes (excluding start and</span>
    <span class="c1"># cur) have nothing in common with any other paths.</span>
    <span class="n">middle</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">curpath</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">shortest</span><span class="p">[</span><span class="n">cur</span><span class="p">]:</span>
        <span class="n">exist_middle</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">exist_middle</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">middle</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
    <span class="k">return</span> <span class="bp">True</span>
</pre></div>


<p><em>Algorithm 1: Ring discovery in a graph.</em></p>
<p><br /></p>
<h4 id="block-building-matching-and-rendering">Block-Building Matching and Rendering</h4>
<p>After discovering blocks, we then match each block to the set of buildings that
lie on top of that block. To do this, we perform a point-in-polygon test for
every vertex of every building onto the polygon defined by every block. While
this test is an approximation of a polygon-in-polygon test, it works well in
practice given the shape of blocks and buildings (there are no extreme, sharp
concavities in the blocks).</p>
<p>Finally, we pick a standard resolution, and transform all blocks and the
buildings on top of them to that fixed resolution output. We the use
Processing[^processing] to render all of the blocks in bulk, creating pairs of
(empty, populated) blocks, rendering blocks in grey and buildings in black. A
few example blocks are shown below in Figure TODO.</p>
<p><br /></p>
<p><img alt="example blocks" src="img/example_blocks.png" /></p>
<p><em>Figure TODO: Three examples from the dataset for task 1: generating buildings
for a block.</em></p>
<p><br /></p>
<p>We scrape five different regions of Seattle and extract all blocks with at
least one building on them, giving a total of 1100 images, which we partition
into train (521) / val (415) / test (164) splits such that the geographic
regions do not overlap between the splits.</p>
<h3 id="task-2-map-regions">Task 2: Map Regions</h3>
<p>The second task we address is: given a region of a map without any buildings,
generate all of the buildings.</p>
<p>This task is simpler from a dataset creation perspective, because it simply
involves rendering two versions of a map: one with most geographic features
rendered except buildings, and the second including buildings as well.</p>
<h4 id="rendering-regions">Rendering Regions</h4>
<p>The main challenge in creating a more realistic region-filling dataset is
accurately rendering a broader range of geographic features. Recall in the
dataset for task 1 that we render only block outlines and buildings. For task
2, we can encode more context render other geographic features such as
walkways, parks, and water. The difficulty in doing so is that these features
are encoded with varying consistency, and at varying levels of abstraction.</p>
<p>Here are a few examples to illustrate how some features are labeled in the
OpenStreetMap data:</p>
<table>
<thead>
<tr>
<th>Tag(s)</th>
<th>Actual Geographic Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>highway: yes</td>
<td>highway</td>
</tr>
<tr>
<td>highway: path</td>
<td>pedestrian walkway</td>
</tr>
<tr>
<td>man_made: pier, source: Yahoo!</td>
<td>walking area</td>
</tr>
<tr>
<td>source: Yahoo!</td>
<td>water</td>
</tr>
<tr>
<td>water: yes</td>
<td>water</td>
</tr>
</tbody>
</table>
<p>After selecting a variety of regions and manually adding mappings between tags,
we end up with seven possible semantic categories per map that we render: (1)
nothing (light grey) (blocks are colored this way), (2) buildings (red
polygons), (3) roads (yellow lines), (4) water (blue polygons), (5) pedestrian
areas (darker grey polygons), (6) pedestrian walkways (darker grey lines), (7)
parks (green polygons). Two example renderings are shown below in Figure TODO:</p>
<p><br /></p>
<p><img alt="example regions" src="img/example_regions.png" /></p>
<p><em>Figure TODO: Two example regions in the dataset for task 2.</em></p>
<p><br /></p>
<h3 id="scraping">Scraping</h3>
<p>The only additional complication with rendering larger map segments is that the
amount of map data required is significantly greater. We address this by
building a small scraping pipeline that walks a given latitude, longitude
geographic region by fixed window sizes (chosen to render well onto a square
image).</p>
<p>We scrape a region encompassing the greater Seattle area: from the Puget Sound
(W) to Sammamish (E), and from SeaTac (S) to Mountlake Terrace (N). This
results in 2967 individual regions, which we filter to include only those with
at least one building. After filtering, 1880 regions remain, which we partition
into train (1680) / val (100) / test (100) splits.</p>
<h2 id="model">Model</h2>
<p>We use the conditional adversarial network, proposed by Isola et al.
(2017).<sup id="fnref:pix2pix"><a class="footnote-ref" href="#fn:pix2pix" rel="footnote">14</a></sup></p>
<p>This model trains two networks, a generator, and a discriminator. The generator
produces candidate output images, and the discriminator attempts to distinguish
between real output images and fake ones. They are trained together so that as
the generator produces more realistic images, the generator also gets better at
distinguishing them.</p>
<p>We use the same model presented in the paper, with the parameters set to be the
same as in the aerial map to topography task.</p>
<h2 id="experimental-results-and-discussion">Experimental Results and Discussion</h2>
<p>For task 1, our model is able to generate fairly convincing blocks filled in
with buildings.</p>
<p><img alt="task 1" src="img/task1-epoch1.png" /></p>
<p><em>Figure TODO: Task 1, epoch 1</em></p>
<p><img alt="task 1 e9" src="img/task1-epoch9.png" /></p>
<p><em>Figure TODO: Task 1, epoch 9</em></p>
<p><img alt="task 1 e191" src="img/task1-epoch191.png" /></p>
<p><em>Figure TODO: Task 1, epoch 9</em></p>
<h2 id="conclusion">Conclusion</h2>
<p>We presented what is, to our knowledge, the first attempt to generate urban
city layouts using machine learning. We collected two "fill in the buildings"
datasets by generating custom renderings from geographic data. We showed that
conditional adversarial network models perform surprisingly well on both tasks,
often producing plausible results even in the absence of truly sufficient
information. Our code is open source,<sup id="fnref:githubmapgen"><a class="footnote-ref" href="#fn:githubmapgen" rel="footnote">16</a></sup> and we would happily make
the data available as well upon request.</p>
<p>In the future, we are eager to explore two lines of work. The first natrual
extension is to improve the current approach. More exhaustive feature mapping
would allow us to more completely capture inconsistently labeled regions like
parks and waterways, and finer grained feature rendering would allow us to
differentiate more <em>waypoint</em> types, such as freeways, arterials, and side
streets. Modifying the model to account for task-specific constraints---such as
that buildings have straight edges and are closed polygons---would produce even
more realistic results. And of course, the current approach provides the road
network as input; an even greater challenge would be to generate the roads as
well (though this would likely move beyond the limits of the current model).</p>
<p>Even more exciting to us is the second line of future work, which is to learn
models inspired by traditional approaches to procedural map generation:
probabilistic grammars. While this approach would be less likely to produce
visually impressive results as quickly as the current model, it would stay more
true to the intent of procedural generation, which is to create systems that
<em>grow</em> rather than <em>fill in</em>. Unsupervised grammar induction, while daunting,
has precedent in natural language processing. Grammar learning would also
transfer more easily to generating other features (roads, water, parks) than
would the model proposed in this paper.</p>
<p><br /></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:vanegas2010modelling">
<p>Carlos A Vanegas, Daniel G Aliaga, Peter Wonka, Pascal
Müller, Paul Waddell, and Benjamin Watson. <em>Modelling the appearance and
behaviour of urban spaces.</em> In Computer Graphics Forum, volume 29, pages 25–42.
Wiley Online Library, 2010.&#160;<a class="footnote-backref" href="#fnref:vanegas2010modelling" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:weber2009interactive">
<p>Basil Weber, Pascal Mu ̈ller, Peter Wonka, and Markus
Gross. <em>Interactive geometric simulation of 4d cities.</em> In Computer Graphics
Forum, volume 28, pages 481–492. Wiley Online Library, 2009.&#160;<a class="footnote-backref" href="#fnref:weber2009interactive" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:weber2009interactive" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:emilien2012procedural">
<p>Arnaud Emilien, Adrien Bernhardt, Adrien Peytavie,
Marie-Paule Cani, and Eric Galin. <em>Procedural generation of villages on
arbitrary terrains.</em> The Visual Computer, 28(6-8):809– 818, 2012.&#160;<a class="footnote-backref" href="#fnref:emilien2012procedural" rev="footnote" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:wu2013inverse">
<p>Fuzhang Wu, Dong-Ming Yan, Weiming Dong, Xiaopeng Zhang, and
Peter Wonka. <em>Inverse procedural modeling of facade layouts.</em> arXiv preprint
arXiv:1308.0419, 2013.&#160;<a class="footnote-backref" href="#fnref:wu2013inverse" rev="footnote" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:vst2010inverse">
<p>Ondrej Sˇt’ava, Bedrich Beneˇs, Radomir Mˇech, Daniel G
Aliaga, and Peter Kriˇstof. <em>Inverse procedural modeling by automatic
generation of l-systems.</em> In Computer Graphics Forum, volume 29, pages 665–674.
Wiley Online Library, 2010.&#160;<a class="footnote-backref" href="#fnref:vst2010inverse" rev="footnote" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:fan2016probabilistic">
<p>Lubin Fan and Peter Wonka. <em>A probabilistic model for
exteriors of residential buildings.</em> ACM Transactions on Graphics (TOG),
35(5):155, 2016.&#160;<a class="footnote-backref" href="#fnref:fan2016probabilistic" rev="footnote" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:martinovic2013bayesian">
<p>Andelo Martinovic and Luc Van Gool. <em>Bayesian
grammar learning for inverse procedural modeling.</em> In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 201–208, 2013.&#160;<a class="footnote-backref" href="#fnref:martinovic2013bayesian" rev="footnote" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:kalogerakis2012probabilistic">
<p>Evangelos Kalogerakis, Siddhartha Chaudhuri,
Daphne Koller, and Vladlen Koltun. <em>A probabilistic model for component-based
shape synthesis.</em> ACM Transactions on Graphics (TOG), 31(4):55, 2012.&#160;<a class="footnote-backref" href="#fnref:kalogerakis2012probabilistic" rev="footnote" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:toshev2010detecting">
<p>Alexander Toshev, Philippos Mordohai, and Ben Taskar.
<em>Detecting and parsing architecture at city scale from range data.</em> In Computer
Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 398–405.
IEEE, 2010.&#160;<a class="footnote-backref" href="#fnref:toshev2010detecting" rev="footnote" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:aliaga2008interactiveA">
<p>Daniel G Aliaga, Bedˇrich Beneˇs, Carlos A Vanegas,
and Nathan Andrysco. <em>Interactive reconfiguration of urban layouts.</em> IEEE
Computer Graphics and Applications, 28(3), 2008.&#160;<a class="footnote-backref" href="#fnref:aliaga2008interactiveA" rev="footnote" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:aliaga2008interactiveB">
<p>Daniel G Aliaga, Carlos A Vanegas, and Bedrich
Benes. <em>Interactive example-based urban layout synthesis.</em> In ACM transactions
on graphics (TOG), volume 27, page 160. ACM, 2008.&#160;<a class="footnote-backref" href="#fnref:aliaga2008interactiveB" rev="footnote" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:vanegas2012procedural">
<p>Carlos A Vanegas, Tom Kelly, Basil Weber, Jan
Halatsch, Daniel G Aliaga, and Pascal Müller. <em>Procedural generation of parcels
in urban modeling.</em> In Computer graphics forum, volume 31, pages 681–690. Wiley
Online Library, 2012.&#160;<a class="footnote-backref" href="#fnref:vanegas2012procedural" rev="footnote" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:nishida2016example">
<p>Gen Nishida, Ignacio Garcia-Dorado, and Daniel G Aliaga.
<em>Example-driven procedural urban roads.</em> In Computer Graphics Forum, volume 35,
pages 5–17. Wiley Online Library, 2016.&#160;<a class="footnote-backref" href="#fnref:nishida2016example" rev="footnote" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:pix2pix">
<p>Philip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros.
<em>Image-to-image translation with conditional adversarial networks.</em> In CVPR,
2017.&#160;<a class="footnote-backref" href="#fnref:pix2pix" rev="footnote" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:Processing">
<p>https://processing.org/&#160;<a class="footnote-backref" href="#fnref:Processing" rev="footnote" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:githubmapgen">
<p>https://github.com/mbforbes/mapgen&#160;<a class="footnote-backref" href="#fnref:githubmapgen" rev="footnote" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</body>
</html>
